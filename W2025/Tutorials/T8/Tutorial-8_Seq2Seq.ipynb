{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jrgreen7/SYSC4906/blob/master/W2025/Tutorials/T8/Tutorial-8_Seq2Seq.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "# Tutorial 8 - seq2seq \n",
    "\n",
    "**Semester:** Winter 2025\n",
    "\n",
    "**Adapted by:** [Kevin Dick](https://kevindick.ai/), [Igor Bogdanov](igorbogdanov@cmail.carleton.ca)\n",
    "\n",
    "**Part I adapted from:** [seq2seq Tutorial](https://github.com/lukas/ml-class/blob/master/videos/seq2seq/train.py) originally from this [Keras Blog](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html).\n",
    "\n",
    "**Part II adapted from:** Clay Woolam <clay@woolam.org> under a BSD License.\n",
    "\n",
    "---\n",
    "\n",
    "## PART 1(a): seq2seq LSTM Model\n",
    "\n",
    "The canonical example of a  sequence-to-sequence (`seq2seq`) learning task is **language translation**. A sequence representing a sentence in one language is encoded into a latent space (an embedded representation) and then decoded into another language.\n",
    "\n",
    "**Neither fixed input/output length:** The input of characters of variable length from a given alphabet needs to somehow be converted into an out variable in length and possibly from an altogether different alphabet.\n",
    "\n",
    "`seq2seq` models generally require **massive amounts** of data to learn their task effectively and this tutorial focuses on a unique example that allows the generation of large amounts of data:\n",
    "\n",
    "### Method: \n",
    "\n",
    "We will generate thousands of **string**-representation of math questions (e.g., `\"39+3\"`) and their target **string**-representation answers (e.g., `\"42\"`). These will be vectorized and used to train an LSTM model that will learn the \"translation\" task of converting a query string from \"question-language\" into a target string in \"answer-language\"!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3984,
     "status": "ok",
     "timestamp": 1636916680450,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 300
    },
    "id": "rnEwIjkhaWoL"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class CharacterTable(object):\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one-hot integer representation\n",
    "    + Decode the one-hot or integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One-hot encode given string C.\n",
    "        # Arguments\n",
    "            C: string, to be encoded.\n",
    "            num_rows: Number of rows in the returned one-hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        \"\"\"Decode the given vector or 2D array to their character output.\n",
    "        # Arguments\n",
    "            x: A vector or a 2D array of probabilities or one-hot representations;\n",
    "                or a vector of character indices (used with `calc_argmax=False`).\n",
    "            calc_argmax: Whether to find the character index with maximum\n",
    "                probability, defaults to `True`.\n",
    "        \"\"\"\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return \"\".join(self.indices_char[x] for x in x)\n",
    "\n",
    "\n",
    "print(\"CharacterTable utility class defined for encoding/decoding characters.\")\n",
    "print(\"This will convert between text strings and one-hot encoded matrices.\")\n",
    "\n",
    "\n",
    "class colors:\n",
    "    ok = \"\\033[92m\"\n",
    "    fail = \"\\033[91m\"\n",
    "    close = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1636916680452,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 300
    },
    "id": "bjYbG-xWaugu"
   },
   "outputs": [],
   "source": [
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 50000\n",
    "DIGITS = 3\n",
    "REVERSE = True\n",
    "\n",
    "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
    "# int is DIGITS.\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "\n",
    "# All the numbers, plus sign and space for padding.\n",
    "chars = '0123456789+ '\n",
    "ctable = CharacterTable(chars)\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "\n",
    "print(f\"Model parameters initialized:\")\n",
    "print(f\"- Training size: {TRAINING_SIZE} examples\")\n",
    "print(f\"- Maximum digits per number: {DIGITS}\")\n",
    "print(f\"- Input reversal: {REVERSE}\")\n",
    "print(f\"- Maximum input length: {MAXLEN} characters\")\n",
    "print(f\"- Character set: '{chars}'\")\n",
    "print(\"Ready to generate training data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7434,
     "status": "ok",
     "timestamp": 1636916687880,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 300
    },
    "id": "Pnpzpk7eaxu3",
    "outputId": "8574366d-d79e-4333-a8cb-325886116de0"
   },
   "outputs": [],
   "source": [
    "print('Generating data...')\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                    for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "    a, b = f(), f()\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN.\n",
    "    q = '{}+{}'.format(a, b)\n",
    "    query = q + ' ' * (MAXLEN - len(q))\n",
    "    ans = str(a + b)\n",
    "    # Answers can be of maximum size DIGITS + 1.\n",
    "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
    "    if REVERSE:\n",
    "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
    "        # space used for padding.)\n",
    "        query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "\n",
    "print('Total addition questions:', len(questions))\n",
    "\n",
    "print(\"\\nSample data (first 5 examples):\")\n",
    "print(\"Question (original) | Question (formatted) | Expected Answer\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(5):\n",
    "    # Get the original question by removing padding and reversing if needed\n",
    "    original_q = questions[i].strip()\n",
    "    if REVERSE:\n",
    "        original_q = original_q[::-1]\n",
    "\n",
    "    print(f\"{original_q} | {questions[i]} | {expected[i]}\")\n",
    "\n",
    "# Also show some statistics about the data\n",
    "print(\"\\nData statistics:\")\n",
    "question_lengths = [len(q.strip()) for q in questions]\n",
    "answer_lengths = [len(a.strip()) for a in expected]\n",
    "print(\n",
    "    f\"Average question length: {sum(question_lengths)/len(question_lengths):.2f} characters\"\n",
    ")\n",
    "print(\n",
    "    f\"Average answer length: {sum(answer_lengths)/len(answer_lengths):.2f} characters\"\n",
    ")\n",
    "print(f\"Shortest question: {min(question_lengths)} characters\")\n",
    "print(f\"Longest question: {max(question_lengths)} characters\")\n",
    "print(f\"Shortest answer: {min(answer_lengths)} characters\")\n",
    "print(f\"Longest answer: {max(answer_lengths)} characters\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1028,
     "status": "ok",
     "timestamp": 1636916688904,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 300
    },
    "id": "I4GDHPKCbGQo",
    "outputId": "626a5172-aff7-452b-d42a-2ac0901f3f4e"
   },
   "outputs": [],
   "source": [
    "print(\"Vectorization...\")\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=bool)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
    "\n",
    "# Display example of vectorization\n",
    "print(\"\\nExample of vectorization:\")\n",
    "print(f\"Original question: '{questions[0]}'\")\n",
    "print(f\"One-hot encoded shape: {x[0].shape} (sequence_length, num_characters)\")\n",
    "\n",
    "# Show a small portion of the one-hot encoding for the first example\n",
    "print(\"\\nFirst few positions of the one-hot encoding:\")\n",
    "sample_indices = min(5, MAXLEN)  # Show first 5 positions or fewer if MAXLEN is smaller\n",
    "sample_chars = min(5, len(chars))  # Show first 5 characters or fewer if chars is smaller\n",
    "print(\"Position | \" + \" \".join(f\"{c:^5}\" for c in chars[:sample_chars]) + \" ...\")\n",
    "for i in range(sample_indices):\n",
    "    print(f\"{i:^8} | \" + \" \".join(f\"{int(x[0][i][j]):^5}\" for j in range(sample_chars)) + \" ...\")\n",
    "\n",
    "print(f\"\\nTotal size of training data: {x.shape[0]} examples\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1636916689111,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 300
    },
    "id": "YUtm0R9xcjl6",
    "outputId": "9b0f0887-bc05-40d0-890a-64869fe1a4bf"
   },
   "outputs": [],
   "source": [
    "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
    "# digits.\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "# Explicitly set apart 10% for validation data that we never train over.\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print(\"Data preparation:\")\n",
    "print(f\"- Total examples: {len(x)}\")\n",
    "print(f\"- Training examples: {len(x_train)} ({len(x_train)/len(x)*100:.1f}%)\")\n",
    "print(f\"- Validation examples: {len(x_val)} ({len(x_val)/len(x)*100:.1f}%)\")\n",
    "\n",
    "# Show a few examples from the training set\n",
    "print(\"\\nSample training examples (after shuffling):\")\n",
    "print(\"Question | Expected Answer\")\n",
    "print(\"-\" * 30)\n",
    "for i in range(3):\n",
    "    # Decode the one-hot encoded data back to strings\n",
    "    q = ctable.decode(x_train[i])\n",
    "    a = ctable.decode(y_train[i])\n",
    "    # Get the original question by removing padding and reversing if needed\n",
    "    original_q = q.strip()\n",
    "    if REVERSE:\n",
    "        original_q = original_q[::-1]\n",
    "    print(f\"{original_q} | {a.strip()}\")\n",
    "\n",
    "print(\"\\nTraining Data Shape:\")\n",
    "print(f\"- Input (x_train): {x_train.shape} (examples, sequence_length, features)\")\n",
    "print(f\"- Output (y_train): {y_train.shape} (examples, sequence_length, features)\")\n",
    "print(\"\\nValidation Data Shape:\")\n",
    "print(f\"- Input (x_val): {x_val.shape}\")\n",
    "print(f\"- Output (y_val): {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5940,
     "status": "ok",
     "timestamp": 1636916695048,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 300
    },
    "id": "X5Gul0MuKheD",
    "outputId": "1b532050-675e-4727-ab9c-f3627551c587"
   },
   "outputs": [],
   "source": [
    "# Try replacing GRU, or SimpleRNN.\n",
    "RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1\n",
    "print(\"Building the seq2seq model...\")\n",
    "model = Sequential()\n",
    "\n",
    "# ENCODER\n",
    "print(\"\\n1. Encoder:\")\n",
    "print(f\"   - Using {RNN.__name__} with {HIDDEN_SIZE} hidden units\")\n",
    "print(\"   - Processes input sequence and produces a fixed-length representation\")\n",
    "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n",
    "# Note: In a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, num_feature).\n",
    "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
    "\n",
    "# BRIDGE\n",
    "print(\"\\n2. Bridge:\")\n",
    "print(f\"   - RepeatVector creates {DIGITS + 1} copies of the encoder output\")\n",
    "print(\"   - This connects the encoder to the decoder\")\n",
    "# As the decoder RNN's input, repeatedly provide with the last output of\n",
    "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "model.add(layers.RepeatVector(DIGITS + 1))\n",
    "\n",
    "# DECODER\n",
    "print(\"\\n3. Decoder:\")\n",
    "print(f\"   - Using {LAYERS} layer(s) of {RNN.__name__} with {HIDDEN_SIZE} hidden units\")\n",
    "print(\"   - Generates the output sequence character by character\")\n",
    "# The decoder RNN could be multiple layers stacked or a single layer.\n",
    "for _ in range(LAYERS):\n",
    "    # By setting return_sequences to True, return not only the last output but\n",
    "    # all the outputs so far in the form of (num_samples, timesteps,\n",
    "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "    # the first dimension to be the timesteps.\n",
    "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "# OUTPUT LAYER\n",
    "print(\"\\n4. Output Layer:\")\n",
    "print(f\"   - TimeDistributed Dense layer with softmax activation\")\n",
    "print(\n",
    "    f\"   - Produces probability distribution over {len(chars)} possible characters at each time step\"\n",
    ")\n",
    "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "# of the output sequence, decide which character should be chosen.\n",
    "model.add(layers.TimeDistributed(layers.Dense(len(chars), activation=\"softmax\")))\n",
    "\n",
    "# COMPILATION\n",
    "print(\"\\n5. Model Compilation:\")\n",
    "print(\"   - Loss: categorical_crossentropy (standard for classification tasks)\")\n",
    "print(\"   - Optimizer: adam (adaptive learning rate)\")\n",
    "print(\"   - Metric: accuracy (percentage of correctly predicted characters)\")\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# MODEL SUMMARY\n",
    "print(\"\\nModel Architecture Summary:\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store val_loss and accuracy for visulization and early stopping\n",
    "train_loss=[]\n",
    "val_loss=[]\n",
    "train_acc=[]\n",
    "val_acc=[]\n",
    "patience=3\n",
    "min_delta=0.001\n",
    "val_loss_increase=0\n",
    "iterations = 2\n",
    "\n",
    "# TRAINING PARAMETERS\n",
    "print(\"\\nTraining Parameters:\")\n",
    "print(f\"- Batch size: {BATCH_SIZE} examples\")\n",
    "print(f\"- Maximum iterations: {iterations} (with early stopping)\")\n",
    "print(f\"- Early stopping patience: {patience} iterations\")\n",
    "print(f\"- Minimum improvement threshold: {min_delta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 249274,
     "status": "ok",
     "timestamp": 1636917134752,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 300
    },
    "id": "Fy5XBlbMcrwk",
    "outputId": "9b4d3421-661c-4d00-fed8-03647f69e9e7"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Train the model each generation and show predictions against the validation\n",
    "# dataset.\n",
    "print(\"\\nStarting training process...\")\n",
    "print(\"Each iteration represents one epoch (full pass through the training data)\")\n",
    "print(\"The model will train until validation loss stops improving\")\n",
    "print(\"After each iteration, we'll show example predictions from the validation set\")\n",
    "\n",
    "for iteration in range(1, iterations):\n",
    "    print()\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Iteration {iteration}/{iterations}\")\n",
    "\n",
    "    # Train for one epoch\n",
    "    train_history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=1,\n",
    "        validation_data=(x_val, y_val),\n",
    "    )\n",
    "\n",
    "    # Track metrics\n",
    "    current_train_loss = train_history.history[\"loss\"][0]\n",
    "    current_val_loss = train_history.history[\"val_loss\"][0]\n",
    "    current_train_acc = train_history.history[\"accuracy\"][0]\n",
    "    current_val_acc = train_history.history[\"val_accuracy\"][0]\n",
    "\n",
    "    train_loss.append(current_train_loss)\n",
    "    val_loss.append(current_val_loss)\n",
    "    train_acc.append(current_train_acc)\n",
    "    val_acc.append(current_val_acc)\n",
    "\n",
    "    # Print current metrics\n",
    "    print(f\"Training loss: {current_train_loss:.4f}, accuracy: {current_train_acc:.2%}\")\n",
    "    print(f\"Validation loss: {current_val_loss:.4f}, accuracy: {current_val_acc:.2%}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if iteration > 1:\n",
    "        loss_improvement = val_loss[iteration - 2] - val_loss[iteration - 1]\n",
    "        if loss_improvement < min_delta:\n",
    "            val_loss_increase += 1\n",
    "            print(\n",
    "                f\"Validation loss not improving. Patience: {val_loss_increase}/{patience}\"\n",
    "            )\n",
    "            if val_loss_increase >= patience:\n",
    "                print(\"Early stopping triggered - validation loss did not improve\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Validation loss improved by {loss_improvement:.6f}\")\n",
    "            val_loss_increase = 0\n",
    "\n",
    "    # Show example predictions\n",
    "    print(\"\\nExample predictions:\")\n",
    "    correct_count = 0\n",
    "    print(\"Question | Target | Prediction | Result\")\n",
    "    print(\"-\" * 50)\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = model.predict(rowx, verbose=0)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=True)\n",
    "\n",
    "        # Get the original question by removing padding and reversing if needed\n",
    "        original_q = q.strip()\n",
    "        if REVERSE:\n",
    "            original_q = original_q[::-1]\n",
    "\n",
    "        result = \"✓\" if correct.strip() == guess.strip() else \"✗\"\n",
    "        if correct.strip() == guess.strip():\n",
    "            correct_count += 1\n",
    "\n",
    "        print(f\"{original_q:10} | {correct.strip():6} | {guess.strip():10} | {result}\")\n",
    "\n",
    "    print(f\"\\nAccuracy on sample: {correct_count/10:.0%}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss, label=\"Training Loss\")\n",
    "plt.plot(val_loss, label=\"Validation Loss\")\n",
    "plt.title(\"Loss over iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_acc, label=\"Training Accuracy\")\n",
    "plt.plot(val_acc, label=\"Validation Accuracy\")\n",
    "plt.title(\"Accuracy over iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Final validation accuracy: {val_acc[-1]:.2%}\")\n",
    "print(f\"Total iterations: {len(train_loss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qd9orV41cuyR"
   },
   "source": [
    "## Part I(b) - Subtraction and Variable Length Input\n",
    "\n",
    "This second implementation demonstrates **subtraction** and permits **variable length** inputs.\n",
    "\n",
    "### Key differences from Part I(a):\n",
    " \n",
    "1. **Operation**: Subtraction instead of addition\n",
    "2. **Character set**: Includes the minus sign \"-\" in addition to digits, plus, and space\n",
    "3. **Digits**: Allows up to 5 digits per number (increased from 3)\n",
    "4. **No reversal**: Input sequences are not reversed in this implementation\n",
    "\n",
    "This implementation shows how the same seq2seq architecture can be adapted to learn different mathematical operations with minimal changes to the model structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 277361,
     "status": "ok",
     "timestamp": 1636917520320,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 300
    },
    "id": "xbxaRmRObH-L",
    "outputId": "07b21378-683d-4db0-844a-9abf01f84996"
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, TimeDistributed, RepeatVector, Dense\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CharacterTable(object):\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One hot encode given string C.\n",
    "        # Arguments\n",
    "            num_rows: Number of rows in the returned one hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return \"\".join(self.indices_char[x] for x in x)\n",
    "\n",
    "\n",
    "__training_size = 50000\n",
    "__digits = 5\n",
    "__hidden_size = 128\n",
    "__batch_size = 128\n",
    "maxlen = __digits + 1 + __digits\n",
    "\n",
    "# All the numbers, plus sign and space for padding.\n",
    "chars = \"0123456789+- \"\n",
    "ctable = CharacterTable(chars)\n",
    "\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "print(\"Generating data...\")\n",
    "while len(questions) < __training_size:\n",
    "    f = lambda: int(\n",
    "        \"\".join(\n",
    "            np.random.choice(list(\"0123456789\"))\n",
    "            for i in range(np.random.randint(1, __digits + 1))\n",
    "        )\n",
    "    )\n",
    "    a, b = f(), f()\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN.\n",
    "    q = \"{}-{}\".format(a, b)\n",
    "    query = q + \" \" * (maxlen - len(q))\n",
    "    ans = str(a - b)\n",
    "    # Answers can be of maximum size DIGITS + 1.\n",
    "    ans += \" \" * (__digits + 1 - len(ans))\n",
    "\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "\n",
    "print(\"Total addition questions:\", len(questions))\n",
    "\n",
    "print(\"\\nSample subtraction data (first 5 examples):\")\n",
    "print(\"Question | Expected Answer\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(5):\n",
    "    print(f\"{questions[i].strip()} | {expected[i].strip()}\")\n",
    "\n",
    "# Add data statistics\n",
    "print(\"\\nSubtraction Data Statistics:\")\n",
    "question_lengths = [len(q.strip()) for q in questions]\n",
    "answer_lengths = [len(a.strip()) for a in expected]\n",
    "print(\n",
    "    f\"Average question length: {sum(question_lengths)/len(question_lengths):.2f} characters\"\n",
    ")\n",
    "print(\n",
    "    f\"Average answer length: {sum(answer_lengths)/len(answer_lengths):.2f} characters\"\n",
    ")\n",
    "print(f\"Shortest question: {min(question_lengths)} characters\")\n",
    "print(f\"Longest question: {max(question_lengths)} characters\")\n",
    "\n",
    "print(\"Vectorization...\")\n",
    "x = np.zeros((len(questions), maxlen, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(questions), __digits + 1, len(chars)), dtype=bool)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, maxlen)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, __digits + 1)\n",
    "\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(__hidden_size, input_shape=(maxlen, len(chars))))\n",
    "model.add(RepeatVector(__digits + 1))\n",
    "model.add(LSTM(__hidden_size, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(len(chars), activation=\"softmax\")))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "print(\"\\nComparison with Addition Model:\")\n",
    "print(\"1. Operation: Subtraction instead of addition\")\n",
    "print(f\"2. Character set: '{chars}' (includes minus sign)\")\n",
    "print(f\"3. Maximum digits: {__digits} (increased from {DIGITS})\")\n",
    "print(\"4. No input reversal in this implementation\")\n",
    "\n",
    "for iteration in range(1, 50):\n",
    "    print()\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Iteration\", iteration)\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=__batch_size,\n",
    "        epochs=1,\n",
    "        validation_data=(x_val, y_val),\n",
    "    )\n",
    "\n",
    "   # Show only three examples per iteration with better formatting\n",
    "    print(\"\\nExample predictions:\")\n",
    "    correct_count = 0\n",
    "    print(\"Question | Expected | Prediction | Result\")\n",
    "    print(\"-\" * 50)\n",
    "    for i in range(3):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = model.predict(rowx, verbose=0)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=True)\n",
    "\n",
    "        # Clean up the strings by removing padding\n",
    "        q_clean = q.strip()\n",
    "        correct_clean = correct.strip()\n",
    "        guess_clean = guess.strip()\n",
    "\n",
    "        result = \"✓\" if correct_clean == guess_clean else \"✗\"\n",
    "        if correct_clean == guess_clean:\n",
    "            correct_count += 1\n",
    "\n",
    "        print(f\"{q_clean:12} | {correct_clean:8} | {guess_clean:10} | {result}\")\n",
    "\n",
    "    print(f\"\\nAccuracy on sample: {correct_count/3:.0%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lA55ljMCTVBr"
   },
   "source": [
    "## Part I (c)- Bonus: the Douglas R. Hofstadter `pq`-system as a `seq2seq` task\n",
    "\n",
    "For those familiar with Dr. Douglas Hofstadter's masterpiece [**(GEB) Gödel, Escher, Bach: and Eternal Golden Braid**](https://www.physixfan.com/wp-content/files/GEBen.pdf), the `pq`-system that Hofstader leverages heavily throughout the book can also be leveraged as an example of a `seq2seq` task.\n",
    "\n",
    "More formally, the `pq`-system has only three disting symbols: `p`, `q`, and `-` and these are used in combination to generate statements/theorems for this system such as:\n",
    "\n",
    "`--p---q-----`\n",
    "\n",
    "`-p-q--`\n",
    "\n",
    "`----------p-q-----------`\n",
    "\n",
    "When you look at these example strings, can you *discern a meaning* for what the symbols `p`, `q`, and `-` stand for? As a human, we might try to identify a pattern within a large number of these statements and hope to deduce a pattern that allows us to generate new and valid statements within this system. \n",
    "\n",
    "### Excerpt from GEB (Chapter II: Isomorphisms Induce Meaning):\n",
    "\n",
    "> Perhaps you have already thought to yourself that the `pq`-theorems are like additions. The string `--p---q-----` is a theorem because 2 plus 3 equals 5. It could even occur to you that the theorem `--p---q-----` is a statement, written in an odd notation, whose meaning is that **2 plus 3 is 5**. Is this a reasonable way to look at things? Well, I deliberately chose 'p' to remind you of 'plus',and 'q' to remind you of 'equals' . . . So, does the string `--p---q-----` actually mean \"2 plus 3 equals 5\"?\n",
    "\n",
    "Aside: GEB is **strongly recommended** to those with deep interests at the intersection of *mathematics, artificial intelligence, philosophy, cognition, musical theory, and the arts.*\n",
    "\n",
    "For the purposes of understanding the utility of `seq2seq` on solving arbitrary **string translation** tasks, this is precisely what a machine learning algorithm must do. \n",
    "\n",
    "**Presented with thousands of examples of valid query strings and their targets, the model learns an internal representation that allows it to correctly map the meaning of an assembly of sybmols into an alternative and valid representation.**\n",
    "\n",
    "---\n",
    "\n",
    "Similar to the examples above that generate example mathematical strings and have the model learn to \"translate\" that input string into its resulting output, we will generate pairs of strings valid in Hofstadter's `pq`-system:\n",
    "\n",
    "**Example:** Input `x=\"---p--\"` with target `y=\"q-----\"`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7216,
     "status": "ok",
     "timestamp": 1636931009753,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 300
    },
    "id": "p28R0U9BaxHT",
    "outputId": "7648447c-cce5-484a-eb92-80fe91f0735d"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, TimeDistributed, RepeatVector, Dense\n",
    "import numpy as np\n",
    "\n",
    "class CharacterTable(object):\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One hot encode given string C.\n",
    "        # Arguments\n",
    "            num_rows: Number of rows in the returned one hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        # print(\"hi\")\n",
    "        # print(x)\n",
    "        # print(x.shape)\n",
    "        print(len(self.indices_char))\n",
    "        return ''.join(self.indices_char[x] for x in x)\n",
    "\n",
    "training_size = 50000\n",
    "digits = 1000 # A \"digit\" here is a dash and we need to allow up to 1000 in length\n",
    "hidden_size = 128\n",
    "batch_size = 128\n",
    "maxlen = digits + 1 + digits\n",
    "\n",
    "# The dash for numbers, p for plus sign, q for equals and space for padding.\n",
    "chars = '-pq '\n",
    "ctable = CharacterTable(chars)\n",
    "\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "print('Generating data...')\n",
    "while len(questions) < training_size:\n",
    "    f = lambda: ''.join('-' for i in range(np.random.randint(1, digits + 1)))\n",
    "    a, b = f(), f()\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN.\n",
    "    q = '{}p{}'.format(a, b)\n",
    "    query = q + ' ' * (maxlen - len(q))\n",
    "    ans = 'q' + '-' * (len(a) + len(b))\n",
    "    # Answers can be of maximum size DIGITS * 2.\n",
    "    ans += ' ' * (maxlen - len(ans))\n",
    "\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "    \n",
    "print('Total addition questions:', len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51188,
     "status": "ok",
     "timestamp": 1636931061520,
     "user": {
      "displayName": "Victoria Ajila",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03727961744353051498"
     },
     "user_tz": 300
    },
    "id": "PmMyXgvUeFE4",
    "outputId": "ecc2c560-a3fd-44ff-d353-e12fbd8a82f6"
   },
   "outputs": [],
   "source": [
    "print('Vectorization...')\n",
    "x = np.zeros((len(questions), maxlen, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(questions), maxlen, len(chars)), dtype=bool)\n",
    "\n",
    "print('Vectorizing questions...')\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, maxlen)\n",
    "print('Vectorising answers...')\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, maxlen)\n",
    "\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "print('Splitting into train and validation sets...')\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print(f'Size train: {len(x_train)}\\tSize val: {len(x_val)}\\nFirst train input: {x_train[0]}\\nFirst train answer: {y_train[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2BmJ6HlzTmoR",
    "outputId": "a200ae19-6be9-4db8-ab2b-55125df0817c"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(hidden_size, input_shape=(maxlen, len(chars))))\n",
    "model.add(RepeatVector(maxlen))\n",
    "model.add(LSTM(hidden_size, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(len(chars), activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "for iteration in range(1, 50):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=1,\n",
    "              validation_data=(x_val, y_val))\n",
    "\n",
    "    # Show only three examples per iteration\n",
    "    for i in range(3):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = model.predict(rowx, verbose=0)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=True)\n",
    "        print('Q', q)\n",
    "        print('T', correct)\n",
    "        if correct == guess:\n",
    "            print('☑')\n",
    "        else:\n",
    "            print('☒')\n",
    "        print(guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGcFh5_Dcddh"
   },
   "source": [
    "# Takeaway Messages\n",
    "* The cannonical example of a `seq2seq` learning task is **language translation**: a seqence represening a sentence in one language is encoded into a latent space (an embedded representation) and then decoded into another language.\n",
    "* In translation, the **input of characters of variable length** and from a **given alphabet** needs to be converted into an **output also variable in length** and possibly from an altogether **different alphabet**.\n",
    "* `seq2seq` models generally require **massive amounts** of data to effectively learn their task."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Tutorial-8_Seq2Seq.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
